{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> HandWritten Text Recognition\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>S Meena Padnekar<br>\n",
    "M.Tech, Department Of Computer Science<br>\n",
    "CUSAT\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>AIM</h4><br>\n",
    "The main objective of this project is to classify the individual words in a handwritten document, so that the handwritten text can be translated to digital form. Convolutional Neural Network is used to classify the words in the handwritten document and for character segmentation, Long Short Term Memory networks (LSTM) is used.  LSTM is an artificial recurrent neural network (RNN) architecture used in the field of deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>DATASET</h4><br>\n",
    "Words in the IAM Handwriting Dataset is used to train the model.<br>\n",
    "IAM Handwriting Dataset is a collection of handwritten passages by several writers. This dataset contain handwritten text over many forms (where a form is an image with lines of text from different writers), sentences, lines, words. The words were then segmented; all associated form label metadata is provided in text file.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>IMPLEMENTATION</h3><br>\n",
    "Total of 80 characters can be recognised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Image Preprocessing Module<br></h3>\n",
    "Grayscale image of size 128x32 is the input to the model.<br>\n",
    "Images are resized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def preprocess(img,imageSize,dataAug=False):\n",
    "\n",
    "    if img is None:\n",
    "        img = np.zeros([imageSize[1],imageSize[2]])\n",
    "    \n",
    "    if dataAug:\n",
    "        stretch = (random.random()-0.5)\n",
    "        wStretched = max(int(img.shape[1] * (1 + stretch)), 1)\n",
    "        img = cv2.resize(img, (wStretched, img.shape[0]))\n",
    "\n",
    "    (wt,ht) = imageSize\n",
    "    (h,w)   = img.shape\n",
    "    fx = w/wt\n",
    "    fy = h/ht\n",
    "    f = max(fx,fy)\n",
    "    newSize = (max(min(wt, int(w / f)), 1), max(min(ht, int(h / f)), 1))\n",
    "    img = cv2.resize(img,newSize)\n",
    "\n",
    "    target = np.ones([ht, wt]) * 255\n",
    "    target[0:newSize[1], 0:newSize[0]]=img\n",
    "\n",
    "    img = cv2.transpose(target)\n",
    "    (m, s) = cv2.meanStdDev(img)\n",
    "\n",
    "    m = m[0][0]\n",
    "    s = s[0][0]\n",
    "    img = img - m\n",
    "    img = img / s if s>0 else img\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>DataLoader Module</h4><br>\n",
    "This is module is used to fetch the images for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "# from imagePreprocessing import preprocess\n",
    "\n",
    "class Sample:\n",
    "    # sample from the dataset\n",
    "    def __init__(self, gtText, filePath):\n",
    "        self.gtText = gtText\n",
    "        self.filePath = filePath\n",
    "\n",
    "class Batch:\n",
    "    # batch containing images and ground truth texts\n",
    "    def __init__(self, gtTexts, imgs):\n",
    "        self.imgs = np.stack(imgs, axis=0)\n",
    "        self.gtTexts = gtTexts\n",
    "\n",
    "class DataLoader:\n",
    "    # loads data which corresponds to IAM format, see: http://www.fki.inf.unibe.ch/databases/iam-handwriting-database\" \n",
    "\n",
    "    def __init__(self, filePath, batchSize, imgSize, maxTextLen):\n",
    "        \"loader for dataset at given location, preprocess images and text according to parameters\"\n",
    "\n",
    "        assert filePath[-1]=='/'\n",
    "\n",
    "        self.dataAugmentation = False\n",
    "        self.currIdx = 0\n",
    "        self.batchSize = batchSize\n",
    "        self.imgSize = imgSize\n",
    "        self.samples = []\n",
    "    \n",
    "        f=open(filePath+'words.txt')\n",
    "        chars = set()\n",
    "        bad_samples = []\n",
    "        bad_samples_reference = ['a01-117-05-02.png', 'r06-022-03-05.png']\n",
    "        for line in f:\n",
    "            # ignore comment line\n",
    "            if not line or line[0]=='#':\n",
    "                continue\n",
    "\n",
    "            lineSplit = line.strip().split(' ')\n",
    "            # assert len(lineSplit) >= 9\n",
    "\n",
    "            # filename: part1-part2-part3 --> part1/part1-part2/part1-part2-part3.png\n",
    "            fileNameSplit = lineSplit[0].split('-')\n",
    "            fileName = filePath + 'words/' + fileNameSplit[0] + '/' + fileNameSplit[0] + '-' + fileNameSplit[1] + '/' + lineSplit[0] + '.png'\n",
    "\n",
    "            # GT text are columns starting at 9\n",
    "            gtText = self.truncateLabel(' '.join(lineSplit[8:]), maxTextLen)\n",
    "            chars = chars.union(set(list(gtText)))\n",
    "\n",
    "            #  check if image is not empty\n",
    "            if not os.path.getsize(fileName):\n",
    "                bad_samples.append(lineSplit[0] + '.png')\n",
    "                continue\n",
    "\n",
    "            # put sample into list\n",
    "            self.samples.append(Sample(gtText, fileName))\n",
    "\n",
    "        # some images in the IAM dataset are known to be damaged, don't show warning for them\n",
    "        if set(bad_samples) != set(bad_samples_reference):\n",
    "            print(\"Warning, damaged images found:\", bad_samples)\n",
    "            print(\"Damaged images expected:\", bad_samples_reference)\n",
    "\n",
    "        # split into training and validation set: 95% - 5%\n",
    "        splitIdx = int(0.95 * len(self.samples))\n",
    "        self.trainSamples = self.samples[:splitIdx]\n",
    "        self.validationSamples = self.samples[splitIdx:]\n",
    "\n",
    "        # put words into lists\n",
    "        self.trainWords = [x.gtText for x in self.trainSamples]\n",
    "        self.validationWords = [x.gtText for x in self.validationSamples]\n",
    "\n",
    "        # number of randomly chosen samples per epoch for training \n",
    "        self.numTrainSamplesPerEpoch = 25000 \n",
    "\n",
    "        # start with train set\n",
    "        self.trainSet()\n",
    "\n",
    "        # list of all chars in dataset\n",
    "        self.charList = sorted(list(chars))\n",
    "\n",
    "\n",
    "    def truncateLabel(self, text, maxTextLen):\n",
    "        # ctc_loss can't compute loss if it cannot find a mapping between text label and input \n",
    "        # labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
    "        # If a too-long label is provided, ctc_loss returns an infinite gradient\n",
    "        cost = 0\n",
    "        for i in range(len(text)):\n",
    "            if i != 0 and text[i] == text[i-1]:\n",
    "                cost += 2\n",
    "            else:\n",
    "                cost += 1\n",
    "            if cost > maxTextLen:\n",
    "                return text[:i]\n",
    "        return text\n",
    "\n",
    "    def trainSet(self):\n",
    "        # switch to randomly chosen subset of training set\n",
    "        self.dataAugmentation = True\n",
    "        self.currIdx = 0\n",
    "        random.shuffle(self.trainSamples)\n",
    "        self.samples = self.trainSamples[:self.numTrainSamplesPerEpoch]\n",
    "\n",
    "\n",
    "    def validationSet(self):\n",
    "        # switch to validation set\n",
    "        self.dataAugmentation = False\n",
    "        self.currIdx = 0\n",
    "        self.samples = self.validationSamples\n",
    "\n",
    "\n",
    "    def getIteratorInfo(self):\n",
    "        # current batch index and overall number of batches\n",
    "        return (self.currIdx // self.batchSize + 1, len(self.samples) // self.batchSize)\n",
    "\n",
    "\n",
    "    def hasNext(self):\n",
    "        # \"iterator\"\n",
    "        return self.currIdx + self.batchSize <= len(self.samples)\n",
    "\n",
    "    \n",
    "    def getNext(self):\n",
    "        # iterator\n",
    "        batchRange = range(self.currIdx, self.currIdx + self.batchSize)\n",
    "        gtTexts = [self.samples[i].gtText for i in batchRange]\n",
    "        imgs = [preprocess(cv2.imread(self.samples[i].filePath, cv2.IMREAD_GRAYSCALE), self.imgSize, self.dataAugmentation) for i in batchRange]\n",
    "        self.currIdx += self.batchSize\n",
    "        return Batch(gtTexts, imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Model</h4><br>\n",
    "Input to the neural network is an image of size 128*32<br>\n",
    "<h5>CNN Layers</h5>\n",
    "There are 5 CNN Layers. These layers are trained to extract relevant features from the image. Kernal filter of size 5x5 is applied on the first 2 layers and then filter of sie 3x3 in the last 3 layers. Non linear RELU function is applied. A max pooling layer is used after each CNN layer such that this layer summarize the image regions and downsize by 2 in each layer<br>\n",
    "The output of CNN layer is feature map of size 32x256<br>\n",
    "<h5>RNN Layers</h5><br>\n",
    "LTSM (a variation of RNN) is used in this model. There are 2 LSTM layers. The feature sequence contain 256 features per time-step. The RNN layer propagates relevant information through this sequence. The out sequence is a matrix of size 32X80. \n",
    "<h5>CTC Layers</h5><br>\n",
    "In training phase, the rnn output is used to calculate the loss and while predicting, it decoded the RNN output matix to get the final text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class DecoderType:\n",
    "    BestPath = 0\n",
    "    BeamSearch = 1\n",
    "    WordBeamSearch = 2\n",
    "\n",
    "\n",
    "class Model:\n",
    "\n",
    "    batchSize = 50\n",
    "    imageSize = (128, 32)\n",
    "    maxTextLen = 32\n",
    "\n",
    "    def __init__(self, charList, decoderType=DecoderType.BestPath, mustRestore=False, dump=False):\n",
    "        self.dump = dump\n",
    "        self.charList = charList\n",
    "        self.decoderType = decoderType\n",
    "        self.mustRestore = mustRestore\n",
    "        self.snapID = 0\n",
    "\n",
    "        self.is_train = tf.placeholder(tf.bool, name='is_train')\n",
    "\n",
    "        self.inputImg = tf.placeholder(tf.float32, shape=(\n",
    "            None, Model.imageSize[0], Model.imageSize[1]))\n",
    "\n",
    "        self.setupCNN()\n",
    "        self.setupRNN()\n",
    "        self.setupCTC()\n",
    "\n",
    "        self.batchesTrained = 0\n",
    "        self.learningRate = tf.placeholder(tf.float32, shape=[])\n",
    "        self.update_op = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "        with tf.control_dependencies(self.update_op):\n",
    "            self.optimizer = tf.train.RMSPropOptimizer(\n",
    "                self.learningRate).minimize(self.loss)\n",
    "\n",
    "        (self.sess, self.saver) = self.setupTF()\n",
    "\n",
    "    def setupCNN(self):\n",
    "        # create cnn layer and return output of these layers\n",
    "        cnnIn4d = tf.expand_dims(input=self.inputImg, axis=3)\n",
    "\n",
    "        # list of parameters for the layers\n",
    "        kernelVals = [5, 5, 3, 3, 3]\n",
    "        featureVals = [1, 32, 64, 128, 128, 256]\n",
    "        strideVals = poolVals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
    "        numLayers = len(strideVals)\n",
    "        # CNN layer\n",
    "        pool = cnnIn4d\n",
    "\n",
    "        for i in range(numLayers):\n",
    "            kernel = tf.Variable(tf.truncated_normal(\n",
    "                [kernelVals[i], kernelVals[i], featureVals[i], featureVals[i + 1]], stddev=0.1))\n",
    "            conv = tf.nn.conv2d(\n",
    "                pool, kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
    "            conv_norm = tf.layers.batch_normalization(\n",
    "                conv, training=self.is_train)\n",
    "            relu = tf.nn.relu(conv_norm)\n",
    "            pool = tf.nn.max_pool(relu, (1, poolVals[i][0], poolVals[i][1], 1), (\n",
    "                1, strideVals[i][0], strideVals[i][1], 1), 'VALID')\n",
    "\n",
    "        self.cnnOut4d = pool\n",
    "\n",
    "    def setupRNN(self):\n",
    "        # create rnn layers and return output of these layers\n",
    "        rnnIn3d = tf.squeeze(self.cnnOut4d, axis=[2])\n",
    "\n",
    "        numHidden = 256\n",
    "        cell = [tf.contrib.rnn.LSTMCell(\n",
    "            num_units=numHidden, state_is_tuple=True) for _ in range(2)]  # 2 layers\n",
    "\n",
    "        stacked = tf.contrib.rnn.MultiRNNCell(cell, state_is_tuple=True)\n",
    "\n",
    "        # bidirectional rnn\n",
    "        ((fw, bw), _) = tf.nn.bidirectional_dynamic_rnn(\n",
    "            cell_fw=stacked, cell_bw=stacked, inputs=rnnIn3d, dtype=rnnIn3d.dtype)\n",
    "\n",
    "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
    "\n",
    "        kernel = tf.Variable(tf.truncated_normal(\n",
    "            [1, 1, numHidden * 2, len(self.charList) + 1], stddev=0.1))\n",
    "\n",
    "        self.rnnOut3d = tf.squeeze(tf.nn.atrous_conv2d(\n",
    "            value=concat, filters=kernel, rate=1, padding='SAME'), axis=[2])\n",
    "\n",
    "    def setupCTC(self):\n",
    "        # calculate loss, decode the word and return\n",
    "        self.ctcIn3d = tf.transpose(self.rnnOut3d, [1, 0, 2])\n",
    "        self.gtText = tf.SparseTensor(tf.placeholder(tf.int64, shape=[None, 2]), tf.placeholder(\n",
    "            tf.int32, shape=[None]), tf.placeholder(tf.int64, shape=[2]))\n",
    "\n",
    "        # loss for batch\n",
    "        self.seqLen = tf.placeholder(tf.int32, [None])\n",
    "        self.loss = tf.reduce_mean(tf.nn.ctc_loss(labels=self.gtText, inputs=self.ctcIn3d, sequence_length=self.seqLen, ctc_merge_repeated=True))\n",
    "\n",
    "        # loss for each element\n",
    "        self.savedCtcInput = tf.placeholder(\n",
    "            tf.float32, shape=[Model.maxTextLen, None, len(self.charList)+1])\n",
    "        self.lossPerElement = tf.nn.ctc_loss(\n",
    "            labels=self.gtText, inputs=self.savedCtcInput, sequence_length=self.seqLen, ctc_merge_repeated=True)\n",
    "\n",
    "        # decoder\n",
    "        if self.decoderType == DecoderType.BestPath:\n",
    "            self.decoder = tf.nn.ctc_greedy_decoder(\n",
    "                inputs=self.ctcIn3d, sequence_length=self.seqLen)\n",
    "        elif self.decoderType == DecoderType.BeamSearch:\n",
    "            self.decoder = tf.nn.ctc_beam_search_decoder(\n",
    "                inputs=self.ctcIn3d, sequence_length=self.seqLen, beam_width=50, merge_repeated=False)\n",
    "        elif self.decoderType == DecoderType.WordBeamSearch:\n",
    "            word_beam_search_module = tf.load_op_library('TFWordBeamSearch.so')\n",
    "\n",
    "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
    "            chars = str().join(self.charList)\n",
    "            wordChars = open('model/wordCharList.txt').read().splitlines()[0]\n",
    "            corpus = open('data/corpus.txt').read()\n",
    "\n",
    "            # decode using the \"Words\" mode of word beam search\n",
    "            self.decoder = word_beam_search_module.word_beam_search(tf.nn.softmax(\n",
    "                self.ctcIn3d, dim=2), 50, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'), wordChars.encode('utf8'))\n",
    "\n",
    "    def setupTF(self):\n",
    "        # print('Python: '+sys.version)\n",
    "        # print('Tensorflow: '+tf.__version__)\n",
    "        # TF session\n",
    "        sess = tf.Session()  \n",
    "        saver = tf.train.Saver(max_to_keep=1,reshape=True)  # saver saves model to file\n",
    "        modelDir = 'model/'     \n",
    "        latestSnapshot = tf.train.latest_checkpoint(modelDir)  # is there a saved model?\n",
    "\n",
    "        # if model must be restored (for inference), there must be a snapshot\n",
    "        if self.mustRestore and not latestSnapshot:\n",
    "            raise Exception('No saved model found in: ' + modelDir)\n",
    "\n",
    "        # load saved model if available\n",
    "        if latestSnapshot:\n",
    "            print('Init with stored values from ' + latestSnapshot)\n",
    "            saver.restore(sess, latestSnapshot)\n",
    "        else:\n",
    "            print('Init with new values')\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        return (sess, saver)\n",
    "\n",
    "    def toSparse(self, texts):\n",
    "    \n",
    "        indices = []\n",
    "        values = []\n",
    "        shape = [len(texts), 0] # last entry must be max(labelList[i])\n",
    "\n",
    "        # go over all texts\n",
    "        for (batchElement, text) in enumerate(texts):\n",
    "            # convert to string of label (i.e. class-ids)\n",
    "            labelStr = [self.charList.index(c) for c in text]\n",
    "            # sparse tensor must have size of max. label-string\n",
    "            if len(labelStr) > shape[1]:\n",
    "                shape[1] = len(labelStr)\n",
    "            # put each label into sparse tensor\n",
    "            for (i, label) in enumerate(labelStr):\n",
    "                indices.append([batchElement, i])\n",
    "                values.append(label)\n",
    "\n",
    "        return (indices, values, shape)\n",
    "\n",
    "    def decoderOutputToText(self, ctcOutput, batchSize):\n",
    "    \n",
    "        # contains string of labels for each batch element\n",
    "        encodedLabelStrs = [[] for i in range(batchSize)]\n",
    "\n",
    "        # word beam search: label strings terminated by blank\n",
    "                    if self.decoderType == DecoderType.`:\n",
    "            blank = len(self.charList)\n",
    "            for b in range(batchSize):\n",
    "                for label in ctcOutput[b]:\n",
    "                    if label == blank:\n",
    "                        break\n",
    "                    encodedLabelStrs[b].append(label)\n",
    "\n",
    "        # TF decoders: label strings are contained in sparse tensor\n",
    "        else:\n",
    "            # ctc returns tuple, first element is SparseTensor\n",
    "            decoded = ctcOutput[0][0]\n",
    "\n",
    "        # go over all indices and save mapping: batch -> values\n",
    "        idxDict = {b: [] for b in range(batchSize)}\n",
    "        for (idx, idx2d) in enumerate(decoded.indices):\n",
    "            label = decoded.values[idx]\n",
    "            batchElement = idx2d[0]  # index according to [b,t]\n",
    "            encodedLabelStrs[batchElement].append(label)\n",
    "\n",
    "        # map labels to chars for all batch elements\n",
    "        return [str().join([self.charList[c] for c in labelStr]) for labelStr in encodedLabelStrs]\n",
    "\n",
    "    def trainBatch(self, batch):\n",
    "        # feed a batch into the NN to train it\n",
    "        numBatchElements = len(batch.imgs)\n",
    "        sparse = self.toSparse(batch.gtTexts)\n",
    "        rate = 0.01 if self.batchesTrained < 10 else (0.001 if self.batchesTrained < 10000 else 0.0001)  # decay learning rate\n",
    "        evalList = [self.optimizer, self.loss]\n",
    "        feedDict = {self.inputImg: batch.imgs, self.gtText: sparse, self.seqLen: [Model.maxTextLen] * numBatchElements, self.learningRate: rate, self.is_train: True}\n",
    "        (_, lossVal) = self.sess.run(evalList, feedDict)\n",
    "        self.batchesTrained += 1\n",
    "        return lossVal\n",
    "\n",
    "    def dumpNNOutput(self, rnnOutput):\n",
    "        # dump the output of the NN to CSV file(s)\n",
    "        dumpDir = 'dump/'\n",
    "        if not os.path.isdir(dumpDir):\n",
    "            os.mkdir(dumpDir)\n",
    "\n",
    "        # iterate over all batch elements and create a CSV file for each one\n",
    "        maxT, maxB, maxC = rnnOutput.shape\n",
    "        for b in range(maxB):\n",
    "            csv = ''\n",
    "            for t in range(maxT):\n",
    "                for c in range(maxC):\n",
    "                    csv += str(rnnOutput[t, b, c]) + ';'\n",
    "                    csv += '\\n'\n",
    "                    fn = dumpDir + 'rnnOutput_'+str(b)+'.csv'\n",
    "                    print('Write dump of NN to file: ' + fn)\n",
    "                    with open(fn, 'w') as f:\n",
    "                        f.write(csv)\n",
    "\n",
    "    def inferBatch(self, batch, calcProbability=False, probabilityOfGT=False):\n",
    "        # feed a batch into the NN to recognize the texts\n",
    "\n",
    "        # decode, optionally save RNN output\n",
    "        numBatchElements = len(batch.imgs)\n",
    "        evalRnnOutput = self.dump or calcProbability\n",
    "        evalList = [self.decoder] + ([self.ctcIn3dTBC] if evalRnnOutput else [])\n",
    "        feedDict = {self.inputImg: batch.imgs, self.seqLen: [\n",
    "        Model.maxTextLen] * numBatchElements, self.is_train: False}\n",
    "        evalRes = self.sess.run(evalList, feedDict)\n",
    "        decoded = evalRes[0]\n",
    "        texts = self.decoderOutputToText(decoded, numBatchElements)\n",
    "\n",
    "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
    "        probs = None\n",
    "        if calcProbability:\n",
    "            sparse = self.toSparse(batch.gtTexts) if probabilityOfGT else self.toSparse(texts)\n",
    "            ctcInput = evalRes[1]\n",
    "            evalList = self.lossPerElement\n",
    "            feedDict = {self.savedCtcInput: ctcInput, self.gtText: sparse, self.seqLen: [Model.maxTextLen] * numBatchElements, self.is_train: False}\n",
    "            lossVals = self.sess.run(evalList, feedDict)\n",
    "            probs = np.exp(-lossVals)\n",
    "\n",
    "        # dump the output of the NN to CSV file(s)\n",
    "        if self.dump:\n",
    "            self.dumpNNOutput(evalRes[1])\n",
    "\n",
    "        return (texts, probs)\n",
    "\n",
    "    def save(self):\n",
    "        # save model to file\n",
    "        self.snapID += 1\n",
    "        self.saver.save(self.sess, 'model/snapshot', global_step=self.snapID)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import os \n",
    "import argparse\n",
    "import cv2\n",
    "\n",
    "import editdistance\n",
    "# from imagePreprocessing import preprocess\n",
    "# from model import Model, DecoderType\n",
    "# from DataProcessing import DataLoader, Batch\n",
    "\n",
    "# File path \n",
    "class FilePath:\n",
    "    input = 'data/test.png'\n",
    "    charList = 'model/charList.txt'\n",
    "    accuracy = 'model/accuracy.txt'\n",
    "    train = 'data/'\n",
    "    corpus = 'data/corpus.txt'\n",
    "\n",
    "def train(model, loader):\n",
    "    epoc = 0\n",
    "    bestCharErrorRate = float('inf')\n",
    "    noImprovement = 0\n",
    "    earlyStopping = 5\n",
    "\n",
    "    while True:\n",
    "        epoc += 1\n",
    "        print('Epoc',epoc)\n",
    "        loader.trainSet()\n",
    "        print('Training Neural Network')\n",
    "        while loader.hasNext():\n",
    "            iterInfo = loader.getIteratorInfo()\n",
    "            Batch = loader.getNext()\n",
    "            loss = model.trainBatch(Batch)\n",
    "            print('Batch : ',iterInfo[0],'/',iterInfo[1],' Loss =',loss)\n",
    "        \n",
    "        print('Validate')\n",
    "        charErrorRate = validate(model,loader)\n",
    "\n",
    "        if charErrorRate < bestCharErrorRate:\n",
    "            print('Increase in accuracy. Saving Model')\n",
    "            bestCharErrorRate = charErrorRate\n",
    "            noImprovement = 0\n",
    "            model.save()\n",
    "            open(FilePath.accuracy,'w').write('Validation Character error rate of the saved model%f%%'%(bestCharErrorRate*100))\n",
    "        else:\n",
    "            print('No increase in Accuracy')\n",
    "            noImprovement +=1\n",
    "        \n",
    "        # stopping if no improving in acc after 5 epoc\n",
    "        if noImprovement>=earlyStopping:\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    loader.validationSet()\n",
    "    numCharErr = 0\n",
    "    numCharTotal = 0\n",
    "    numWordOK = 0\n",
    "    numWordTotal = 0\n",
    "    while loader.hasNext():\n",
    "        iterInfo = loader.getIteratorInfo()\n",
    "        print('Batch:', iterInfo[0],'/', iterInfo[1])\n",
    "        batch = loader.getNext()\n",
    "        (recognized,_) = model.inferBatch(batch)\n",
    "\n",
    "        for i in range(len(recognized)):\n",
    "            numWordOK += 1 if batch.gtText[i] == recognized[i] else 0 \n",
    "            numWordTotal +=1\n",
    "            dist = editdistance.eval(recognized[i],batch.gtText[i])\n",
    "            numCharErr += dist\n",
    "            numCharTotal += len(batch.gtText[i])\n",
    "            print('[OK]' if dist==0 else '[ERR:%d]' % dist,'\"' + batch.gtTexts[i] + '\"', '->', '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    charErrorRate = numCharErr/numCharTotal if numCharTotal !=0 else 0\n",
    "    wordAccuracy = numWordOK/numWordTotal if numWordTotal !=0 else 0\n",
    "    print('Character error rate: %f%%. Word accuracy: %f%%.' % (charErrorRate*100.0, wordAccuracy*100.0))\n",
    "    return charErrorRate\n",
    "\n",
    "def recognize(model,InImage):\n",
    "    img = preprocess(cv2.imread(InImage,cv2.IMREAD_GRAYSCALE),Model.imageSize)\n",
    "    batch = Batch(None,[img])\n",
    "    (recognized,probability) = model.inferBatch(batch, True)\n",
    "    print('Recognized:', '\"' + recognized[0] + '\"')\n",
    "    print('Probability:', probability[0])\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--train', help='train the Neural network', action='store_true')\n",
    "    parser.add_argument('--validate', help='test the Neural network', action='store_true')\n",
    "    parser.add_argument('--beamsearch', help='use beam search instead of best path decoding', action='store_true')\n",
    "    parser.add_argument('--wordbeamsearch', help='use word beam search instead of best path decoding', action='store_true')\n",
    "    parser.add_argument('--dump', help='store the NN weights', action='store_true')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    decoderType = DecoderType.BestPath\n",
    "    if args.beamsearch:\n",
    "        decoderType = DecoderType.BeamSearch\n",
    "    elif args.wordbeamsearch:\n",
    "        decoderType = DecoderType.WordBeamSearch\n",
    "\n",
    "    if args.train or args.validate :\n",
    "        # load training data\n",
    "        # execute training and validation\n",
    "        loader = DataLoader(FilePath.train,Model.batchSize,Model.imageSize,Model.maxTextLen)\n",
    "        open(FilePath.charList, 'w').write(str().join(loader.charList))\n",
    "        open(FilePath.corpus,'w').write(str(' ').join(loader.trainWords + loader.validationWords))\n",
    "\n",
    "        if args.train:\n",
    "            # training\n",
    "            model = Model(loader.charList,decoderType)\n",
    "            train(model, loader)\n",
    "        elif args.validate:\n",
    "            # validate\n",
    "            model = Model(loader,charList,decoderType,mustRestore=True)\n",
    "            validate(model, loader)\n",
    "    else:\n",
    "        print(open(FilePath.accuracy).read())\n",
    "        model = Model(open(FilePath.charList).read(), decoderType, mustRestore=True, dump=args.dump)\n",
    "        recognize(model,FilePath.input)\n",
    "            \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>RESULT</h4><br>\n",
    "A model that can recognize words of size upto 32 characters was  implemented.<br>\n",
    "Validation Character error rate of the saved model is 16.507551%<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>REFERENCE</h4><br>\n",
    "<p>1. Handwritten Text Recognition, Batuhan Balci, Dan Saadati, Dan Shiferaw\n",
    "<h4>LINKS</h4><br>\n",
    "Full code : https://github.com/smeenapadnekar/HandWritten-Text-Recognition <br>\n",
    "Dataset : http://www.fki.inf.unibe.ch/databases/iam-handwriting-database <br>\n",
    "TUTORIALS : https://www.tensorflow.org/tutorials/images/cnn\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
